{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae385ca",
   "metadata": {},
   "source": [
    "# Customer Segmentation and CLV Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b23cd",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa65179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(\"Online Retail.xlsx\", engine='openpyxl')\n",
    "    print(\"Data loaded successfully from Excel file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Online Retail.xlsx' not found.\")\n",
    "    print(\"Please download the dataset and place it in the same directory as this notebook.\")\n",
    "    df = pd.DataFrame(columns=[\"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\"])\n",
    "\n",
    "if not df.empty:\n",
    "    print(df.head())\n",
    "    print(\"\\nShape of the data:\", df.shape)\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac9d3f",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    initial_rows = df.shape[0]\n",
    "    df.dropna(subset=['CustomerID'], inplace=True)\n",
    "    print(f\"Dropped {initial_rows - df.shape[0]} rows with missing CustomerID.\")\n",
    "\n",
    "    df['CustomerID'] = df['CustomerID'].astype(int).astype(str)\n",
    "\n",
    "    df['IsCancelled'] = df['InvoiceNo'].astype(str).str.startswith('C')\n",
    "    print(f\"Found {df['IsCancelled'].sum()} cancelled order lines.\")\n",
    "    \n",
    "    df = df[df['Quantity'] > 0]\n",
    "    print(f\"Removed rows with negative or zero quantity.\")\n",
    "\n",
    "    df = df[df['UnitPrice'] > 0]\n",
    "    print(f\"Removed rows with zero or negative unit price.\")\n",
    "\n",
    "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Dropped {initial_rows - df.shape[0]} duplicate rows.\")\n",
    "    \n",
    "    print(f\"\\nCleaned data shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    df_clean = preprocess_data(df)\n",
    "    print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97ec28",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clean' in locals() and not df_clean.empty:\n",
    "    df_time = df_clean.set_index('InvoiceDate').resample('M')['TotalPrice'].sum()\n",
    "    fig1 = px.line(df_time, x=df_time.index, y='TotalPrice', title='Total Revenue Over Time',\n",
    "                   labels={'TotalPrice': 'Total Revenue', 'InvoiceDate': 'Month'})\n",
    "    fig1.show()\n",
    "\n",
    "    top_countries = df_clean.groupby('Country')['TotalPrice'].sum().nlargest(10).reset_index()\n",
    "    fig2 = px.bar(top_countries, x='Country', y='TotalPrice', title='Top 10 Countries by Revenue',\n",
    "                  color='Country')\n",
    "    fig2.show()\n",
    "    \n",
    "    df_clean['Log_UnitPrice'] = np.log1p(df_clean['UnitPrice'])\n",
    "    df_clean['Log_Quantity'] = np.log1p(df_clean['Quantity'])\n",
    "\n",
    "    fig3 = make_subplots(rows=1, cols=2, subplot_titles=(\"Distribution of UnitPrice (Log)\", \"Distribution of Quantity (Log)\"))\n",
    "    fig3.add_trace(go.Histogram(x=df_clean['Log_UnitPrice'], name='UnitPrice'), row=1, col=1)\n",
    "    fig3.add_trace(go.Histogram(x=df_clean['Log_Quantity'], name='Quantity'), row=1, col=2)\n",
    "    fig3.update_layout(title_text=\"Distributions of Price and Quantity (Log-Transformed)\", showlegend=False)\n",
    "    fig3.show()\n",
    "    \n",
    "    top_products = df_clean.groupby('Description')['Quantity'].sum().nlargest(20).reset_index()\n",
    "    fig4 = px.bar(top_products, x='Quantity', y='Description', title='Top 20 Most Sold Products', orientation='h')\n",
    "    fig4.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "    fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7af697",
   "metadata": {},
   "source": [
    "### 3.1. Cohort Retention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c80195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(x): return dt.datetime(x.year, x.month, 1)\n",
    "\n",
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    return year * 12 + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0664dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clean' in locals() and not df_clean.empty:\n",
    "    df_clean['InvoiceMonth'] = df_clean['InvoiceDate'].apply(get_month)\n",
    "    \n",
    "    grouping = df_clean.groupby('CustomerID')['InvoiceMonth']\n",
    "    df_clean['CohortMonth'] = grouping.transform('min')\n",
    "    \n",
    "    invoice_month_int = get_date_int(df_clean, 'InvoiceMonth')\n",
    "    cohort_month_int = get_date_int(df_clean, 'CohortMonth')\n",
    "    \n",
    "    df_clean['CohortIndex'] = invoice_month_int - cohort_month_int\n",
    "\n",
    "    cohort_data = df_clean.groupby(['CohortMonth', 'CohortIndex'])['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
    "    \n",
    "    cohort_counts = cohort_data.pivot_table(index='CohortMonth',\n",
    "                                          columns='CohortIndex',\n",
    "                                          values='CustomerID')\n",
    "\n",
    "    cohort_sizes = cohort_counts.iloc[:, 0]\n",
    "    retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
    "    retention = retention.round(3) * 100\n",
    "    \n",
    "    retention.index = retention.index.strftime('%Y-%m')\n",
    "    \n",
    "    plt.figure(figsize=(18, 10))\n",
    "    sns.heatmap(retention, annot=True, fmt='.1f', cmap='viridis_r', \n",
    "                yticklabels=retention.index, \n",
    "                cbar_kws={'label': 'Retention Rate (%)'})\n",
    "    plt.title('Customer Retention Cohort Heatmap', fontsize=20, pad=20)\n",
    "    plt.xlabel('Months Since First Purchase', fontsize=14)\n",
    "    plt.ylabel('First Purchase Month (Cohort)', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c2ef1",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering (RFM & CLV Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_features(df_clean):\n",
    "    train_df = df_clean[(df_clean['InvoiceDate'] >= '2010-12-01') & (df_clean['InvoiceDate'] < '2011-09-01')]\n",
    "    \n",
    "    target_df = df_clean[(df_clean['InvoiceDate'] >= '2011-09-01') & (df_clean['InvoiceDate'] < '2011-12-01')]\n",
    "    \n",
    "    print(f\"Training data: {train_df.shape[0]} rows\")\n",
    "    print(f\"Target data: {target_df.shape[0]} rows\")\n",
    "\n",
    "    snapshot_date = dt.datetime(2011, 9, 1)\n",
    "    \n",
    "    customer_features = train_df.groupby('CustomerID').agg(\n",
    "        Recency=('InvoiceDate', lambda x: (snapshot_date - x.max()).days),\n",
    "        Frequency=('InvoiceNo', 'nunique'),\n",
    "        Monetary=('TotalPrice', 'sum'),\n",
    "        FirstPurchaseDate=('InvoiceDate', 'min'),\n",
    "        NumUniqueProducts=('StockCode', 'nunique')\n",
    "    ).reset_index()\n",
    "\n",
    "    customer_features['AOV'] = customer_features['Monetary'] / customer_features['Frequency']\n",
    "    \n",
    "    customer_features['Tenure'] = (snapshot_date - customer_features['FirstPurchaseDate']).dt.days\n",
    "    \n",
    "    customer_features['PurchaseFrequencyRate'] = customer_features['Frequency'] / (customer_features['Tenure'] + 1)\n",
    "    \n",
    "    clv_target = target_df.groupby('CustomerID')['TotalPrice'].sum().reset_index()\n",
    "    clv_target.rename(columns={'TotalPrice': 'CLV_3M'}, inplace=True)\n",
    "\n",
    "    clv_df = pd.merge(customer_features, clv_target, on='CustomerID', how='left')\n",
    "    clv_df['CLV_3M'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"\\nCustomer-level feature dataframe created:\")\n",
    "    print(clv_df.head())\n",
    "    return clv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clean' in locals() and not df_clean.empty:\n",
    "    clv_df = create_customer_features(df_clean)\n",
    "    print(clv_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ffbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clv_df' in locals():\n",
    "    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Recency Distribution\", \n",
    "                                                         \"Frequency Distribution\", \n",
    "                                                         \"Monetary Distribution\",\n",
    "                                                         \"3M-CLV Target Distribution\"))\n",
    "    fig.add_trace(go.Histogram(x=clv_df['Recency'], name='Recency'), row=1, col=1)\n",
    "    fig.add_trace(go.Histogram(x=clv_df['Frequency'][clv_df['Frequency'] < 100], name='Frequency (filtered)'), row=1, col=2)\n",
    "    fig.add_trace(go.Histogram(x=clv_df['Monetary'][clv_df['Monetary'] < 5000], name='Monetary (filtered)'), row=2, col=1)\n",
    "    fig.add_trace(go.Histogram(x=clv_df['CLV_3M'][clv_df['CLV_3M'] > 0], name='CLV (non-zero)'), row=2, col=2)\n",
    "    fig.update_layout(title_text=\"Distributions of Raw RFM & CLV Features (some filtered for visibility)\", height=700)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f7b71",
   "metadata": {},
   "source": [
    "## 5. RFM Analysis and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rfm_segments(df):\n",
    "    rfm = df.copy()\n",
    "    \n",
    "    rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5, 4, 3, 2, 1]).astype(int)\n",
    "    \n",
    "    rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "    rfm['M_Score'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5]).astype(int)\n",
    "    \n",
    "    rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
    "    \n",
    "    segment_map = {\n",
    "        r'[4-5][4-5][4-5]': 'Champions',\n",
    "        r'[3-5][3-5][1-3]': 'Loyal Customers',\n",
    "        r'[4-5][1-3][4-5]': 'Potential Loyalists (High Spend, Low Freq)',\n",
    "        r'[4-5][1-3][1-3]': 'New Customers',\n",
    "        r'[3-5][1-3][1-3]': 'Promising (Recent, Low Freq/Spend)',\n",
    "        r'[3-4][4-5][1-5]': 'Loyal Customers (Needs Attention)',\n",
    "        r'[2-3][2-3][2-3]': 'Needs Attention',\n",
    "        r'[2-3][1-5][4-5]': 'At Risk (High Spend, Slipping)',\n",
    "        r'[1-2][3-5][3-5]': 'At Risk (High Value, Not Recent)',\n",
    "        r'[1-2][1-2][3-5]': 'Hibernating (High Spend, Lost)',\n",
    "        r'[1-2][1-5][1-2]': 'Lost'\n",
    "    }\n",
    "    \n",
    "    rfm['Segment'] = rfm['RFM_Score'].replace(segment_map, regex=True)\n",
    "    \n",
    "    rfm['Segment'] = rfm['Segment'].apply(lambda x: 'Lost' if x.isdigit() else x)\n",
    "\n",
    "    return rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clv_df' in locals():\n",
    "    rfm_df = create_rfm_segments(clv_df)\n",
    "    print(rfm_df[['CustomerID', 'RFM_Score', 'Segment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05aaf7a",
   "metadata": {},
   "source": [
    "### 5.1. Visualizing RFM Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rfm_df' in locals():\n",
    "    segment_counts = rfm_df['Segment'].value_counts().reset_index()\n",
    "    segment_counts.columns = ['Segment', 'Count']\n",
    "    \n",
    "    fig1 = px.bar(segment_counts, x='Segment', y='Count', color='Segment', \n",
    "                  title='Customer Segment Distribution')\n",
    "    fig1.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "    fig1.show()\n",
    "\n",
    "    fig2 = px.scatter(rfm_df, x='Recency', y='Frequency', color='Segment', \n",
    "                      title='Recency vs. Frequency by Segment', \n",
    "                      hover_data=['CustomerID', 'Monetary', 'Segment'],\n",
    "                      log_y=True, log_x=True, \n",
    "                      color_discrete_sequence=px.colors.qualitative.Plotly)\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rfm_df' in locals():\n",
    "    segment_summary = rfm_df.groupby('Segment').agg(\n",
    "        Recency=('Recency', 'mean'),\n",
    "        Frequency=('Frequency', 'mean'),\n",
    "        Monetary=('Monetary', 'mean'),\n",
    "        CLV_3M=('CLV_3M', 'mean'),\n",
    "        Count=('CustomerID', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    metrics_to_plot = ['Recency', 'Frequency', 'Monetary', 'CLV_3M']\n",
    "    summary_scaled = segment_summary.copy()\n",
    "    summary_scaled[metrics_to_plot] = scaler.fit_transform(summary_scaled[metrics_to_plot])\n",
    "    summary_scaled['Recency'] = 1 - summary_scaled['Recency']\n",
    "    \n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    for i, row in summary_scaled.iterrows():\n",
    "        fig3.add_trace(go.Scatterpolar(\n",
    "            r=row[metrics_to_plot].values,\n",
    "            theta=metrics_to_plot,\n",
    "            fill='toself',\n",
    "            name=row['Segment']\n",
    "        ))\n",
    "\n",
    "    fig3.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title=\"Normalized Segment Characteristics (Recency is inverted)\"\n",
    "    )\n",
    "    fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb963cc",
   "metadata": {},
   "source": [
    "## 6. Customer Lifetime Value (CLV) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rfm_df' in locals():\n",
    "    features = ['Recency', 'Frequency', 'Monetary', 'AOV', 'Tenure', 'NumUniqueProducts', 'PurchaseFrequencyRate']\n",
    "    target = 'CLV_3M'\n",
    "\n",
    "    X = rfm_df[features]\n",
    "    y = rfm_df[target]\n",
    "\n",
    "    y_log = np.log1p(y)\n",
    "\n",
    "    X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(f\"Training features shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Test features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6274e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    \n",
    "    y_pred[y_pred < 0] = 0\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    return r2, mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093051d",
   "metadata": {},
   "source": [
    "### 6.1. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90771e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = {}\n",
    "\n",
    "if 'X_train_scaled' in locals():\n",
    "    print(\"Training Linear Regression...\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train_log)\n",
    "    y_pred_log_lr = lr.predict(X_test_scaled)\n",
    "    r2_lr, mae_lr, rmse_lr = evaluate_model(y_test_log, y_pred_log_lr)\n",
    "    model_performance['Linear Regression'] = {'R2': r2_lr, 'MAE': mae_lr, 'RMSE': rmse_lr}\n",
    "    print(\"Linear Regression Done.\")\n",
    "\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)\n",
    "    rf.fit(X_train_scaled, y_train_log)\n",
    "    y_pred_log_rf = rf.predict(X_test_scaled)\n",
    "    r2_rf, mae_rf, rmse_rf = evaluate_model(y_test_log, y_pred_log_rf)\n",
    "    model_performance['Random Forest'] = {'R2': r2_rf, 'MAE': mae_rf, 'RMSE': rmse_rf}\n",
    "    print(\"Random Forest Done.\")\n",
    "\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', \n",
    "                               n_estimators=100, \n",
    "                               learning_rate=0.1, \n",
    "                               max_depth=5, \n",
    "                               random_state=42, \n",
    "                               n_jobs=-1)\n",
    "    xgb_reg.fit(X_train_scaled, y_train_log)\n",
    "    y_pred_log_xgb = xgb_reg.predict(X_test_scaled)\n",
    "    r2_xgb, mae_xgb, rmse_xgb = evaluate_model(y_test_log, y_pred_log_xgb)\n",
    "    model_performance['XGBoost'] = {'R2': r2_xgb, 'MAE': mae_xgb, 'RMSE': rmse_xgb}\n",
    "    print(\"XGBoost Done.\")\n",
    "\n",
    "    print(\"\\n--- Model Performance Comparison ---\")\n",
    "    perf_df = pd.DataFrame(model_performance).T\n",
    "    print(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998eec15",
   "metadata": {},
   "source": [
    "### 6.2. Model Interpretation (Feature Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3084c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_reg' in locals():\n",
    "    importance = pd.DataFrame({'Feature': features, 'Importance': xgb_reg.feature_importances_})\n",
    "    importance = importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    fig = px.bar(importance, x='Importance', y='Feature', orientation='h', \n",
    "                 title='Feature Importance for CLV Prediction (XGBoost)')\n",
    "    fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_reg' in locals():\n",
    "    y_test_true = np.expm1(y_test_log)\n",
    "    y_test_pred = np.expm1(y_pred_log_xgb)\n",
    "    y_test_pred[y_test_pred < 0] = 0\n",
    "\n",
    "    # FIXED: Use min(1000, len(y_test_true)) to avoid sampling more than available\n",
    "    sample_size = min(1000, len(y_test_true))\n",
    "    plot_df = pd.DataFrame({'True CLV': y_test_true, 'Predicted CLV': y_test_pred}).sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    fig = px.scatter(plot_df, x='True CLV', y='Predicted CLV', \n",
    "                     title=f'True vs. Predicted 3-Month CLV (XGBoost) - Sample of {sample_size}',\n",
    "                     opacity=0.5,\n",
    "                     trendline='ols', trendline_color_override='red')\n",
    "    fig.add_shape(type='line', x0=0, y0=0, x1=plot_df['True CLV'].max(), y1=plot_df['True CLV'].max(), \n",
    "                  line=dict(color='blue', dash='dash'), name='Perfect Prediction')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39927d00",
   "metadata": {},
   "source": [
    "## 7. Clustering (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rfm_df' in locals():\n",
    "    cluster_features = ['Recency', 'Frequency', 'Monetary', 'AOV', 'Tenure']\n",
    "    \n",
    "    cluster_df = rfm_df[cluster_features].copy()\n",
    "    cluster_df['Recency'] = np.log1p(cluster_df['Recency'])\n",
    "    cluster_df['Frequency'] = np.log1p(cluster_df['Frequency'])\n",
    "    cluster_df['Monetary'] = np.log1p(cluster_df['Monetary'])\n",
    "    cluster_df['AOV'] = np.log1p(cluster_df['AOV'])\n",
    "    cluster_df['Tenure'] = np.log1p(cluster_df['Tenure'])\n",
    "    \n",
    "    cluster_scaler = StandardScaler()\n",
    "    cluster_scaled = cluster_scaler.fit_transform(cluster_df)\n",
    "    \n",
    "    wcss = []\n",
    "    k_range = range(2, 11)\n",
    "    \n",
    "    print(\"Running Elbow Method...\")\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "        kmeans.fit(cluster_scaled)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    print(\"Elbow Method Done.\")\n",
    "\n",
    "    fig = px.line(x=k_range, y=wcss, title='Elbow Method for Optimal K',\n",
    "                  labels={'x': 'Number of Clusters (k)', 'y': 'WCSS'})\n",
    "    fig.add_vline(x=5, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Optimal k = 5\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster_scaled' in locals():\n",
    "    optimal_k = 5\n",
    "    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)\n",
    "    rfm_df['KMeans_Cluster'] = kmeans.fit_predict(cluster_scaled)\n",
    "    \n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    centers_df = pd.DataFrame(cluster_centers, columns=cluster_features)\n",
    "    centers_df['Cluster'] = centers_df.index\n",
    "    print(\"K-Means Cluster Centers (Scaled)\")\n",
    "    print(centers_df)\n",
    "    \n",
    "    print(\"\\n--- K-Means vs RFM Segment Crosstab ---\")\n",
    "    crosstab = pd.crosstab(rfm_df['KMeans_Cluster'], rfm_df['Segment'])\n",
    "    print(crosstab)\n",
    "    \n",
    "    rfm_df['KMeans_Cluster_Str'] = 'Cluster ' + rfm_df['KMeans_Cluster'].astype(str)\n",
    "    cluster_summary = rfm_df.groupby('KMeans_Cluster_Str').agg(\n",
    "        Recency=('Recency', 'mean'),\n",
    "        Frequency=('Frequency', 'mean'),\n",
    "        Monetary=('Monetary', 'mean'),\n",
    "        CLV_3M=('CLV_3M', 'mean')\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e9a6a",
   "metadata": {},
   "source": [
    "## 8. Final Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'xgb_reg' in locals():\n",
    "    os.makedirs('outputs/models', exist_ok=True)\n",
    "    \n",
    "    model_path = 'outputs/models/clv_model.pkl'\n",
    "    joblib.dump(xgb_reg, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    scaler_path = 'outputs/models/scaler.pkl'\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved to {scaler_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
